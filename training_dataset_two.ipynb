{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true,"authorship_tag":"ABX9TyMESw///8L9yG39Lf4m/aLI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Importing hugging face transformers and libraries\n","\n"],"metadata":{"id":"12vHhGWVBMoO"}},{"cell_type":"code","source":["!pip install transformers datasets pandas scikit-learn\n","\n","# Import necessary libraries\n","import torch\n","import pandas as pd\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from sklearn.model_selection import train_test_split  # Import train_test_split\n","from torch.utils.data import DataLoader\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bhNBxdhyBSCi","executionInfo":{"status":"ok","timestamp":1730554061474,"user_tz":-540,"elapsed":8060,"user":{"displayName":"Kseniya Chadovich","userId":"12835703330727720958"}},"outputId":"d5d8e770-f6b2-4fd2-efcf-330aade203ff"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"]}]},{"cell_type":"markdown","source":["# Setting the device"],"metadata":{"id":"ZhhTEs13BwMa"}},{"cell_type":"code","source":["\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {DEVICE}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IOfYeQ4oBzKU","executionInfo":{"status":"ok","timestamp":1730554221491,"user_tz":-540,"elapsed":269,"user":{"displayName":"Kseniya Chadovich","userId":"12835703330727720958"}},"outputId":"9567b4a2-6cf8-426f-dc19-7ea84681a9d4"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"markdown","source":["# Loading tokenizer and model from hf"],"metadata":{"id":"WiX3uZRXCFpV"}},{"cell_type":"code","source":["# 1. Load Tokenizer and Model for BERT\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n","model.to(DEVICE)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K3L2Ei8XCQy2","executionInfo":{"status":"ok","timestamp":1730554223129,"user_tz":-540,"elapsed":543,"user":{"displayName":"Kseniya Chadovich","userId":"12835703330727720958"}},"outputId":"953295e9-dc52-4324-fc7f-5c0af029c334"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSdpaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["# Checkpoint Dir"],"metadata":{"id":"2feyIUceDrKU"}},{"cell_type":"code","source":["# Create a directory for saving checkpoints\n","CHECKPOINT_DIR = '/content/checkpoints'\n","os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n"],"metadata":{"id":"v2DS_A_6DyxR","executionInfo":{"status":"ok","timestamp":1730554228271,"user_tz":-540,"elapsed":276,"user":{"displayName":"Kseniya Chadovich","userId":"12835703330727720958"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# Load clean training data"],"metadata":{"id":"eshRmuijD5-k"}},{"cell_type":"code","source":["!pip install datasets\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Load the dataset from Hugging Face\n","splits = {'train': 'train.jsonl', 'test': 'test.jsonl'}\n","df = pd.read_json(\"hf://datasets/SetFit/hate_speech_offensive/\" + splits[\"train\"], lines=True)\n","\n","# Map the labels: 0 for \"neither\" (nothate) and 1 for anything offensive (hate)\n","df['label'] = df['label_text'].apply(lambda x: 0 if x == 'neither' else 1)\n","\n","# Keep only the 'text' and 'label' columns to match the previous format\n","df = df[['text', 'label']]\n","\n","# Remove mentions, hashtags, links, and \"RT\" from the text\n","df['text'] = df['text'].str.replace(r'@\\w+', '', regex=True)   # Remove mentions\n","df['text'] = df['text'].str.replace(r'#\\w+', '', regex=True)   # Remove hashtags\n","df['text'] = df['text'].str.replace(r'http\\S+|www\\S+|t.co/\\S+', '', regex=True)  # Remove links\n","df['text'] = df['text'].str.replace(r'\\bRT\\b', '', regex=True)\n","df['text'] = df['text'].str.replace(':', '', regex=True)  # Remove \"RT\" for retweets\n","\n","# Drop rows with any missing values (if any)\n","df.dropna(inplace=True)\n","\n","# Split into training and validation sets\n","train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","# Display the cleaned dataset format\n","print(train_df.head(15))\n","\n","\n","# Convert the DataFrames to lists for tokenization\n","train_texts = train_df['text'].tolist()\n","train_labels = train_df['label'].tolist()\n","val_texts = val_df['text'].tolist()\n","val_labels = val_df['label'].tolist()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TjveKYTkQEjX","executionInfo":{"status":"ok","timestamp":1730622892505,"user_tz":-540,"elapsed":24668,"user":{"displayName":"Kseniya Chadovich","userId":"12835703330727720958"}},"outputId":"12b6ea43-ad92-4dcd-868c-98e46326a16e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n","Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n","                                                    text  label\n","6179         Got a badass bitch just bouncin on my dick       1\n","19957  ,,,,,, |  | Blonde tranny whore gets rammed an...      1\n","3672   \"You look cute today :)\".. \"Really?!\" Helll no...      1\n","3721          Baby girl your pussy looking so vacant....      1\n","22652    got bitches that look like Bulldogs instead lol      1\n","17647                                       true dat nig      1\n","13904    : Lmao you gotta chill  : You bitches still fat      1\n","10491  Same people bitchin about jetpacks and shit in...      1\n","1396             : 50' 2nd yellow card issued for Temple      0\n","5473    : When a girl bring up my \"hoes\" when I'm try...      1\n","4154    : Call my son \"nigger\": even respectable whit...      1\n","16563              I spy a couple thirsty hoes on insta.      1\n","13631             : all these bitches fake if you ask me      1\n","7687   I learnt about birds as a child. Best time to ...      0\n","4863                      : So cute! :)  : Shy bunny...       0\n"]}]},{"cell_type":"markdown","source":["# Tokenization"],"metadata":{"id":"VZIfkpC3RESm"}},{"cell_type":"code","source":["# Tokenize the text data\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n"],"metadata":{"id":"PpsjtRG6RHni"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Prep"],"metadata":{"id":"HcNxjTQqRZsP"}},{"cell_type":"code","source":["class HateSpeechDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","# Create datasets\n","train_dataset = HateSpeechDataset(train_encodings, train_labels)\n","val_dataset = HateSpeechDataset(val_encodings, val_labels)\n"],"metadata":{"id":"N3jeQcuoRfOB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# Data Loaders"],"metadata":{"id":"ctHpbjzQRl5S"}},{"cell_type":"code","source":["train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"],"metadata":{"id":"TTF12qp2Rq96"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training Setup"],"metadata":{"id":"O4a_c95lR0FW"}},{"cell_type":"code","source":["from transformers import AdamW\n","\n","# Set up the optimizer\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lAuScWYaR23u","executionInfo":{"status":"ok","timestamp":1730096996950,"user_tz":-540,"elapsed":416,"user":{"displayName":"Kseniya Chadovich","userId":"12835703330727720958"}},"outputId":"56238f4b-b0a0-4287-9711-135e34c2a53e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["# Training Loop"],"metadata":{"id":"nTBqVXPsR8yg"}},{"cell_type":"code","source":["from transformers import AdamW\n","from torch.cuda.amp import GradScaler, autocast\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","scaler = GradScaler()\n","\n","num_epochs = 3\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch in train_loader:\n","        # Move input tensors to the GPU\n","        for key in batch:\n","            batch[key] = batch[key].to(DEVICE)\n","\n","        optimizer.zero_grad()\n","\n","        # Automatic Mixed Precision\n","        with autocast():\n","            outputs = model(**batch)\n","            loss = outputs.loss\n","\n","        # Scale the loss for mixed precision\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        # Free up memory\n","        torch.cuda.empty_cache()\n","\n","    print(f\"Epoch {epoch + 1} finished.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7BahSA-_SDL7","executionInfo":{"status":"ok","timestamp":1730098431365,"user_tz":-540,"elapsed":179607,"user":{"displayName":"Kseniya Chadovich","userId":"12835703330727720958"}},"outputId":"142f3b21-c433-41fa-a32d-1bc9fbaae3bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","<ipython-input-29-c6f40cf45cac>:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","<ipython-input-29-c6f40cf45cac>:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 finished.\n","Epoch 2 finished.\n","Epoch 3 finished.\n"]}]},{"cell_type":"markdown","source":["# Saving the model"],"metadata":{"id":"1KmDxRr26fkr"}},{"cell_type":"code","source":["# After your training loop\n","output_dir = \"/drive/MyDrive/training_v1\"  # Specify your save path\n","\n","# Save the model\n","model.save_pretrained(output_dir)\n","\n","# Save the tokenizer\n","tokenizer.save_pretrained(output_dir)\n","\n","print(f\"Model and tokenizer saved to {output_dir}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qDnUbTi66jiz","executionInfo":{"status":"ok","timestamp":1730099030935,"user_tz":-540,"elapsed":2156,"user":{"displayName":"Kseniya Chadovich","userId":"12835703330727720958"}},"outputId":"f512df55-93ba-4284-8493-a13015face79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model and tokenizer saved to /drive/MyDrive/training_v1\n"]}]}]}